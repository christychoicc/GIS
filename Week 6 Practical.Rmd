---
title: "Week 6 Practical"
output: html_document
---

# Detecting Spatial Patterns

In this analysis we will analyse the patterns of Blue Plaques — you will see these placed on around the UK linking the buildings of the present to people of the past. - ***For any given London Borough, are the Blue Plaques within that borough distributed randomly or do they exhibit some kind of dispersed or clustered pattern?***

-   We want to find out are plaques at random? clustered? or dispersed?

```{r}
#clear memory
rm(list = ls()) 
gc()
```

```{r}
# load packages
library(spatstat)
library(here)
library(sp)
library(tmap)
library(sf) #sf = data frame + geometry + CRS
library(tmaptools)
```

## Setting Up Data

First, read in London Borough Boundaries (polygon)

-   We set the study area so that we can know where points occurs later and define "random" or "clustered"!

-   We also need this to clip Blue Plaques to London and tag each plaque with the borough

-   And other things...

```{r}
#London Borough Boundaries
LondonBoroughs <- st_read(here::here("Week 1 Practical Data", "statistical-gis-boundaries-london", "ESRI", "London_Borough_Excluding_MHW.shp"))

#or you can use this:
#LondonBoroughs <- st_read("https://opendata.arcgis.com/datasets/8edafbe3276d4b56aec60991cbddda50_4.geojson")
```

Second, we pull out London using `str_detect()` function from the `stringr` package in combination with `filter()` from `dplyr` (again!).

We will look for the bit of the district code that relates to London (E09) from the ‘lad15cd’ column data frame of our `sf` object

By doing this, we aim to achieve:

-   Filter study area (London - E09 only)

-   Project to meters: transform to 27700 (British National Grid so distances/ areas are in meters)

```{r}
library(stringr)
library(dplyr)

BoroughMap <- LondonBoroughs %>%
  dplyr::filter(str_detect(GSS_CODE, "^E09")) %>%
  st_transform(., 27700)

qtm(BoroughMap)
```

Ok, now lets give a quick sanity check before we start any spatial stats!

Good!! we see:

-   Row length =33, we got all 33 London Boroughs

-   Geometry type: MULTIPOLYGON: 33, all boroughs are multipolygons

-   Area distribution:

    -   min=3.15ha (That's tiny! should be City of London!), max=15,013ha(Bigger! maybe it's Bromley?)

    -   strong skew (mean 4,832\>median 358) (mean\>median is a strong hint of right-skewed!! = mean is sensitive to big values)

```{r}
summary(BoroughMap)
```

Next Step: Blue Plaques (Points)

In the above, we have loaded in the borough polygons, now we need the point patterns, so, now, we need to get the location of all Blue Plaques in the City (directly from the web):

```{r}
BluePlaques <- st_read("https://s3.eu-west-2.amazonaws.com/openplaques/open-plaques-london-2018-04-08.geojson")%>%
  st_transform(.,27700)
```

```{r}
summary(BluePlaques)
```

```{r}
#plot the blue plaques in the city
tmap_mode("plot")

tm_shape(BoroughMap) +
  tm_polygons(fill_alpha = 0.5)+
tm_shape(BluePlaques) +
  tm_dots(fill = "blue", size=0.1)
```

### Data Cleaning

In the above plotted map, we can see that there is at least 1 Blue Plaque that falls outside of the Borough boundaries!! These "Errant Plaques" (aka plaque points that don’t lie **inside** your study window (the London borough polygons) will cause problems with our analysis!!

So, we need to clip the Plaques to the boundaries

First step –\> is to remove any Plaques with the same grid reference as this will cause problems later on in the analysis...

```{r}
#remove duplicate rows
library(tidyverse)

library(sf)
BluePlaques <- distinct(BluePlaques)
# distinct() --> compares all columns in the data (aka it scans from top to bottom then keeps the first time it sees a particular row, then drops any later rows that are identical across every column)
```

### Spatial Subsetting

Selecting features **based on where they are** in space (their geometry), not based on their attribute values.

In the below code, the second operator is blank (, ) –\> this controls which all columns/ attributes are kept

The second position in `[rows, cols]` is left **blank** (`[,]`), which in base R means **keep all columns** (all attributes). You’re not dropping any fields now; you’ll tidy later with dplyr if needed.

```{r}
#drops any plaque points that don’t touch/lie within a London borough
BluePlaquesSub <- BluePlaques[BoroughMap,]

#plot the blue plaques in the city
tmap_mode("plot")

#check to see that they've been removed
tm_shape(BoroughMap) +
  tm_polygons(fill_alpha = 0.5)+
tm_shape(BluePlaquesSub) +
  tm_dots(fill = "blue", size=0.1)
```

When we spatial subset data like this there are different topological relations we can specify. The default is intersects (st_intersects: keeps point that touch/lie in the borough),

But we could use BluePlaquesSub \<- BluePlaques[BoroughMap, , op = st_within], with the operator or op set to st_within (keeps only points **strictly inside** the borough polygons (no border-touch)), to identify points completely within the borough outline.

Any topological operator from Week5 Mapping can be used here!

```{r}
# add sparse=false to get the complete matrix.
# this is the same as select by location in QGIS
# filter from dplyr is the same as select by attribute 
intersect_indices <-st_intersects(BoroughMap, BluePlaques)
```

### Spatial Clipping

**cutting geometries to a boundary** so that only the part **inside** a chosen area remains.

-   If you clip **polygons/lines**, their shapes are **trimmed** to the boundary.

<!-- -->

-   If you clip **points**, it’s effectively the same as spatial **subsetting** (points outside are dropped; points inside are unchanged).

-   **REMEMBER !! CRS must match** before clipping (`st_transform()`) !!

-   Do this to keep data inside your study area (aka London only)

### Key Advice

When do I want to do what?

1.  Spatial Subsetting
    -   Select points or polygons in a polygon (aka selecting data by location)

    -   What it does: keep or drop whole features based on a spatial test (inside, intersects within X meters, etc.)
2.  Spatial Clipping
    -   Determine where datasets overlap (or touch, or don’t overlap) and extract those parts
3.  Spatial Joining
    -   Join two spatial datasets together

    -   Which can use spatial subsetting functions as the default is `st_intersects()`. This function joins spatial data.
4.  Selecting Data by attributes
    -   Filtering or selecting rows / columns with dplyr

### Study Area

1.  Subset our sf object to pull out a borough we are interested in (choosing Harrow in this practical)

```{r}
#extract the borough

# select by attribute
Harrow <- BoroughMap %>%
  filter(., NAME=="Harrow") #keep rows where the NAME column equals "Harrow"

#Check to see that the correct borough has been pulled out
tm_shape(Harrow) +
  tm_polygons(col = NA, fill_alpha = 0.5)
```

2.  Clip our Blue Plaques so we have a subset (only plaques that lie inside Harrow) of just those that fall within the borough or interest

```{r}
#clip the data to our single borough
BluePlaquesSub <- BluePlaques[Harrow,]



#check that it's worked
tm_shape(Harrow) +
  tm_polygons(fill_alpha = 0.5)+
tm_shape(BluePlaquesSub) +
  tm_dots(fill = "blue", size=0.1)
```

3.  Set up done! Can start our analysis using `spatstat`. Set up a window:
    -   Create an observation window for `spatstat` to carry out its analysis within – we will set this to the extent of the Harrow Boundary

        -   Observation window – **exact area in which points *could* have been observed** and in which you will test “random vs clustered vs dispersed.”

        -   **In this practical – Define the window** as the Harrow polygon → tell spatstat “analyse **within this shape**.”

```{r}
#now set a window as the borough boundary
window <- as.owin(Harrow)
plot(window)
```

spatstat (built for spatial statistics) uses its own classes (which needs to be converted from you original sf objects before running tests like Ripley's K, nearest-neighbour ...)

It does not work directly with Spatial PolygonsDataFrames or sf objects we are used to.

For a **Point Pattern Analysis** (**statistical tests on point locations to detect randomness/ cluster/ dispersion**), we need to create a ppp object:

-   **`owin`** = an *observation window* (the study region/shape (where points would occur))

-   **`ppp`** = a *point pattern* (points **plus** the window, units, and optional marks)

```{r}
#older-but-still-common pattern
#create a sp object: sf -> sp (SpatiaPointDataFrame)
BluePlaquesSub<- BluePlaquesSub %>%
  as(., 'Spatial')

#create a spatstat ppp object
#by passing those x/y vectors and the window (owin of Harrow)
BluePlaquesSub.ppp <- ppp(x=BluePlaquesSub@coords[,1], 
                          # extract X (easting vector) from sp
                          y=BluePlaquesSub@coords[,2], 
                          # extract Y (northing vector(how far east a point is from the grid’s false origin, in metres)) from sp
                          window=window) 
                          # an owin made from Harrow --> tells spatstat the exact region in which points are considered/analysed.
```

(Optional) To try understand the above code, run the below for elements of the code:

```{r}
BluePlaquesSub@coords[,1]

#easting (X) → how far east the point is, in metres.
#northing (Y) → how far north the point is, in metres.
#BluePlaquesSub@coords[, 1] means: “give me all rows (:) from column 1 only” → the vector of eastings.

#output values → distances in metres from a the grid's “false origin”
#in this case: Harrow's easting of ~510,000-520,000, so the output values are sensible
```

Ok now look at the new ppp object:

-   **visualising the point pattern** you built—an essential sanity check before running stats

```{r}
BluePlaquesSub.ppp %>%
  plot(.,pch=16,cex=0.5, 
       # call its plot method (plot.ppp)
       # Plot BluwPlaquesSub point pattern, which plot.ppp draws the window(Harrow Boundary) and overlay the points
       # pch= point symbol: 16 = filled circle
       # cex= point size scaled to 50%
       main="Blue Plaques Harrow")
```

## Point Pattern Analysis

### Kernel Density Estimation (KDE)

One way to summarise your point data is to plot the density of your points under a window called a 'Kernel'

-   KDE turns a set of points into a **smooth surface** that estimates the **intensity** (points per unit area) at every location.

<!-- -->

-   size and shape of the Kernel affects the density pattern produced

-   but it is very easy to produce a KDE map from a ppp object using the density() function.

-   Benefits of a KDE

    -   **Hotspot mapping:** see where plaques concentrate (exploratory “heatmap” with real units).

    -   **Compare areas:** higher intensity ≈ more plaques **per area** 每個區域嘅斑塊多啲, not just more plaques overall 而唔係淨係整體上多啲斑塊.

    -   **Inputs to modeling:** use KDE as a covariate 協變量 or to guide inhomogeneous tests 用嚟指導唔均勻嘅測試.

    -   **Scale-aware:** change bandwidth to examine patterns at different distances (e.g., 200 m vs 1 km).

    -   Helpful step to use in projects: **After subsetting to a borough:** make a KDE map to **visualise hotspots** (town centres, historic streets).

```{r}
BluePlaquesSub.ppp %>%  # 1) take your spatstat point pattern (ppp)
  density(., sigma=500) %>%  # 2) compute a kernel density estimate with 500 m bandwidth
  plot()  # 3) plot the resulting raster surface
```

***Sigma Values***

-   Sigma – the bandwidth that controls how far each point's influence spreads

-   It sets the diameter of the Kernel (in the units your map is in — in this case, as we are in British National Grid the units are in metres)

***Try different sigma values – density estimate changes!!***

-   small sigma value – narrow/ tall bumps

    -   lots of **small circles** around each point; each circle is a peak.

    -   Bumps **overlap less**, so you see **distinct little peaks** centered right on individual points.

-   large sigma value – wide/low bumps

    -   Instead of many sharp peaks, you get **broad hills/hotspots** showing **neighborhood-scale** patterns, not dot-by-dot wiggles.

    -   circles **blend** into big blobs; individual points stop standing out.

***How to choose the sigma value:***

-   Street-/neighbourhood-scale hotspots → **300–600 m**

-   Borough-wide trends → **800–1500 m**

```{r}
BluePlaquesSub.ppp %>%
  density(., sigma=1000) %>%
  plot()
```

## Quadrant Analysis

Are distribution of points in our study area differs from 'complete spatial randomness' - CSR (careful not CRS!!) ?

Basic test of CSR is using a quadrat analysis - using the `quadrat count` function in `spatstat`

What it does it do (simple):

-   **Divide** the study window into a grid of equal “quadrats” (cells).

-   **Count** points in each cell.

-   **Compare** the observed counts to what you’d expect under **Complete Spatial Randomness (CSR)**

***What are we trying to do?***

We want to decide whether the Blue Plaques (points) inside a borough look **random**, **clustered**, or **dispersed**.

-   The idea is to compare what we **observed** to what we’d expect if points were placed **completely at random** inside the same borough. That “random” reference is **Complete Spatial Randomness (CSR)**, modeled by a **Poisson process**.

***How does a quadrat analysis work? (Basically mentioned above but in easy understanding here)***

1.  Lay a grid over the borough (window)
2.  Count plaques per quadrat
3.  Summarise Counts into a frequency table
4.  Estimate the Poisson Mean, lambda
    -   lambda –\> meanshow many plaques per cell on average

    -   Under CSR, the number of points in an equal-area quadrat follows **Poisson(λ)**, where **λ = average plaques per quadrat**.
5.  Compute expected counts under Poisson
6.  Compare observed vs expected
    -   See if your *observed* is **close to** the *expected* Poisson(λ) (→ **random**), or **too different** (→ **patterned**).

```{r}
#First plot the point patterns
#Approach by drawing the window first, then the plaque points on top
plot(BluePlaquesSub.ppp, #BluePlaquesSub.ppp is a ppp (points + window)
     pch=16,
     cex=0.5, 
     main="Blue Plaques in Harrow")

#now count the points in that fall in a 6 x 6
#grid overlaid across the windowBluePlaquesSub.ppp2<-BluePlaquesSub.ppp %>%
BluePlaquesSub.ppp %>%
  quadratcount(.,nx = 6, ny = 6)%>%  
  #6 by 6 grid over your window (Harrow) and count how many points fall in each cell
    plot(., add=T, col="red")
    # add = TRUE --> “draw on top of the current plot,” not a new one.
```

We want to know whether or not there is any kind of spatial patterning associated with the Blue Plaques in London –\> this means **comparing our observed distribution of points with a statistically likely (Complete Spatial Random) distribution** (將我哋觀察到嘅點分佈同一個統計上可能嘅（完全空間隨機）分佈比較), based on the Poisson distribution.

-   the CSR (Poisson) null tells us what "random" should look like given your window and density so you can say whether what you see is more clustered/ regular than chance

-   **Observed** = 真實藍牌位置（你見到嘅點）

-   **CSR（Poisson）** = 假設喺同一個邊界入面、同平均密度下，點係**完全隨機**、互相**獨立**。

Using the same `quadratcount()` function again (for the same sized grid) we can save the results into a table:

```{r}
#run the quadrat count
#Qcount is the observed frequency distribution of counts per quadrat
Qcount <- BluePlaquesSub.ppp %>%
  quadratcount(.,nx = 6, ny = 6) %>% #count how many plaques fall in each grid cell
  as.data.frame() %>% #turn that object into a data frame: each row = one quarat (cell) , and make a column
  dplyr::count(Var1=Freq)%>% 
  dplyr::rename(Freqquadratcount=n) #rename to Freqquadratcount
```

Check the data type in the first column - if it is factor –\> need to convert it to numeric

```{r}
Qcount %>% 
  summarise_all(class)
```

Ok now we have created a frequency table (Qcount)

Next we need to calculate our expected values –\> using the Poisson Distribution formula for calculating expected probabilities.

-   Equation

    -   `x` is the number of occurrences

    -   `λ` is the mean number of occurrences

    -   `e` is a constant- 2.718

-   Inside the code:

    -   `Var1` = (in Qcount) = the count of quadrats per quadrat

    -   `Freqquadratcount` = the **number of quadrats** that had that count.

```{r}
#calculate the total blue plaques (Var * Freq)
sums <- Qcount %>%
  mutate(total = Var1 * Freqquadratcount) %>%   # plaques contributed by each row
  dplyr::summarise(across(everything(), sum))%>%  # sum over all rows
  dplyr::select(-Var1) #drop Var1 cus we summed everything in one row it no longer makes sense keeping a Var1 column after summing la on9

#calculate lambda
lambda<- Qcount%>%
  mutate(total = Var1 * Freqquadratcount)%>%
  dplyr::summarise(across(everything(), sum)) %>%
  mutate(lambda=total/Freqquadratcount) %>%  #repeats the same totals then this
  dplyr::select(lambda)%>%
  pull(lambda) 

  # lambda is the mean rate of points 
  # (aka average number of plaques per quadrat, 
  # it sets the poisson lambda you compare your observed counts against!!)
```

Calculate expected using the Poisson formula from above,

k is the number of blue plaques counted in a square and is found in the first column of our table

**What is done in the below code? :**

-   computing the expected number of quadrats with plaques under a Poisson model (CSR)

-   then plot observed vs expected to compare the two distributions

```{r}
QCountTable <- Qcount %>%
  mutate(Pr=((lambda^Var1)*exp(-lambda))/factorial(Var1))%>%
  #now calculate the expected counts based on our total number of plaques
  #and save them to the table
  mutate(Expected= (round(Pr * sums$Freqquadratcount, 0)))

#Compare the frequency distributions of the observed and expected point patterns
plot(c(1,5),c(0,14), type="n",
xlab="Number of Blue Plaques (Red=Observed,Blue=Expected)", 
     ylab="Frequency of Occurances")
points(QCountTable$Freqquadratcount, 
       col="Red", 
       type="o", 
       lwd=3)
points(QCountTable$Expected, col="Blue", 
       type="o", 
       lwd=3)
```

Comparing the observed and expected frequencies for our quadrant counts

-   can observe that they both have higher frequency counts at the lower end — something reminiscent (令人聯想到) of a Poisson distribution.

-   this could indicate that for this particular set of quadrants, our pattern is close to Complete Spatial Randomness (i.e. no clustering or dispersal of points).

To check this,

We can use the `quadrat.test()` function, built into `spatstat`.

-   This uses a **Chi Squared test** to **compare the observed and expected frequencies** for each quadrant (rather than for quadrant bins, as we have just computed above).

    -   A Chi-Squared test determines if there is an association between two categorical variables. The higher the Chi-Squared value, the greater the difference.

    -   If the p-value of our Chi-Squared test is \< 0.05, then we can reject a null hypothesis that says “there is no pattern - i.e. complete spatial randomness - in our data” (think of a null-hypothesis as the opposite of a hypothesis that says our data exhibit a pattern).

    -   If our p-value is \> 0.05 then this indicates that we have CSR and there is no pattern in our points. If it is \< 0.05, this indicates that we do have clustering in our points.

```{r}
#runs the quadrat test for CSR on your Harrow plaques using a 6×6 grid
teststats <- quadrat.test(BluePlaquesSub.ppp, nx = 6, ny = 6)
```

```{r}
# layer two plots, the points and the quadrat residuals from the test above
plot(BluePlaquesSub.ppp,pch=16,cex=0.5, main="Blue Plaques in Harrow")
plot(teststats, add=T, col = "red") #overlay quadrat grid with numbers in each cell
```

```{r}
teststats
```

Here, the **p-value = 0.2594** –\> implying complete spatial randomness (CSR)!!

Potential Problems with Quadrant Analysis:

-   Look back two codes above, there is a warning message of observed counts to be very small , this may affect the accuracy of the quadrant test

-   Poisson Distribution –\> only describes observed occurances that are counted in integers –\> where our occurrences =0 (i.e. not observed) can raise issues.

-   MAUP (modified areal unit problem)

If try running a quadrant analysis for different grid arrangements (2x2 or 3x3 or 10x10...) What will happen???

## Ripley's K

Using the `spatstat` package using the `kest()` function.

Tells you whether points are **clustered**, **random**, or **regular** at different **distance scales**.

Compare the observed distribution of points with the Poisson random model for a whole range of different distance radii

```{r}
K <- BluePlaquesSub.ppp %>%
  Kest(., correction="border") %>%
  plot()
```

```{r}
# we can also extract the plot into a dataframe
Kval <- as.data.frame(Kest(BluePlaquesSub.ppp, correction = "Ripley"))
```

-   Definition in English: Ripley’s K value for any circle radius r =

    -   The average density of points for the **entire study region (of all locations)** λ=(n/Πr2))

    -   Multiplied by the sum of the distances dij between all points within that search radius, see [Dixon page 2](https://www3.nd.edu/~mhaenggi/ee87021/Dixon-K-Function.pdf) and [Amgad et al. 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144404)

    -   Divided by the total number of points, n

    -   I = 1 or 0 depending if dij<r

The plot for K has a number of elements that are worth explaining.

-   The *Kpois(r)* line in **Red is the theoretical value** of K for each distance window (r) under a Poisson assumption of Complete Spatial Randomness.

-   The **Black line is the estimated values** of K accounting for the effects of the edge of the study area.

The correction specifies how points towards the edge are dealt with

-   In this case, border means that points towards the edge are ignored for the calculation but are included for the central points. [Section 2.1, here](https://www.statistics.gov.hk/wsc/IPS031-P2-S.pdf) explains the different options.

-   How to read it:

    -   **Black above red** ⇒ **more neighbours** within rrr than CSR ⇒ **clustering** at that scale.

    -   **Black below red** ⇒ **fewer neighbours** within rrr than CSR ⇒ **regularity/dispersion** at that scale.

    -   **Black ≈ red** ⇒ looks **random** at that scale.

        -   Where the value of K falls above the line, the data appear to be clustered (聚集) at that distance. Where the value of K is below the line, the data are dispersed (分散).

            -   From the graph, we can see that up until distances of around 1300 metres, (**Black above red**) Blue Plaques appear to be clustered (聚集) in Harrow, however, at around 1500 m, the distribution appears random (**black and red meets**) and then dispersed(分散) between about 1600 and 2100 meters (**black below red**).

## Alternatives to Ripley's K

Not compulsory - if interested refer to <https://andrewmaclachlan.github.io/CASA0005repo/detecting-spatial-patterns.html#alternatives-to-ripleys-k>

## Density-based spatial clustering of applications with noise: DBSCAN

DBSCAN discovers clusters in space (be this physical space or variable space) - tell us **WHERE** in our area of interest the clusters are occurring.

aka the clustering algorithm that finds groups of points that are close and dense, and labels the rest as noise. The core idea is

-   a circus of radius (eps) around each point

-   if a point has at least minpts neighbours inside that circle its a core point

-   唔靠近任何 circle 嘅 points 會變成 **noise (points that don’t belong to any cluster)**（aka unclustered）

```{r}
library(fpc)
```

Now we carry out a DBSCAN analysis of blue plaques in my borough to see if there are any clusters present:

```{r}
#first check the coordinate reference system of the Harrow spatial polygon:
st_geometry(BoroughMap)
```

DBSCAN requires you to input two parameters:

1.  *Epsilon* - this is the radius within which the algorithm with search for clusters
2.  *MinPts* - this is the minimum number of points that should be considered a cluster

Based on the results of the Ripley’s K analysis earlier,

-   We can see that we are getting clustering up to a radius of around 1200m, with the largest bulge (最大嘅凸起/ gap) in the graph at around 700m.

-   Therefore, 700m is probably a good place to start and we will begin by searching for clusters of at least 4 points…

```{r}
#first extract the points from the spatial points data frame
BluePlaquesSubPoints <- BluePlaquesSub %>%
  coordinates(.)%>%
  as.data.frame()

#now run the dbscan analysis
db <- BluePlaquesSubPoints %>%
  fpc::dbscan(.,eps = 700, MinPts = 4)

#now plot the results
plot(db, BluePlaquesSubPoints, main = "DBSCAN Output", frame = F)
plot(BoroughMap$geometry, add=T)
```

You could also use `kNNdistplot()` from the `dbscan` package to find a suitable eps value based on the ‘knee’ in the plot…

```{r}
# used to find suitable eps value based on the knee in plot
# k is no of nearest neighbours used, use min points
library(dbscan)

BluePlaquesSubPoints%>%
  dbscan::kNNdistplot(.,k=4)
```

-   This plot shows for each point the average distance to the k neighbours (對於每個 point，睇下佢最近嘅neighbours), which are then plotted in ascending order.

-   The knee is where this value (of distance to neighbours) increases.

So the DBSCAN analysis shows that for these values of eps and MinPts there are three clusters in the area I am analysing.

Now of course the plot above is a little basic and doesn’t look very aesthetically pleasing. We can always produce a much nicer plot by extracting the useful information from the DBSCAN output and use `ggplot2` to produce a cool map…

```{r}
library(ggplot2)
```

Our new db object contains lots of info including cluster each set of point coordinates belongs to, whether the point is a seed point or a border point etc. So, lets get a summary by calling the object:

```{r}
db
```

```{r}
db$cluster
```

We can now add this cluster membership info back into our dataframe:

-   **Cluster membership** = the label that says **which cluster each point belongs to** (or that it’s **noise**)

```{r}
BluePlaquesSubPoints<- BluePlaquesSubPoints %>%
  mutate(dbcluster=db$cluster)
```

Next we are going to create some convex hull polygons to wrap around the points in our clusters. This means we will create a smallest polygon that encloses the set of points of each cluster. We will

-   convert this dataframe to an sf object

-   make convex hulls around the clusters

-   plot

```{r}
# convert the data frame to sf
BluePlaquesSubPoints_sf <- st_as_sf(BluePlaquesSubPoints, 
                                    coords = c("coords.x1", "coords.x2"), 
                                    crs = 27700)

# make the convex hulls around the cluster points
chull_polygons <- BluePlaquesSubPoints_sf %>%
  # remove any points not in a cluster
  filter(dbcluster>0)%>%
  group_by(dbcluster) %>%
  summarise(geometry = st_combine(geometry)) %>%  # combine points
  mutate(geometry = st_convex_hull(geometry)) %>% # convex hull
  st_as_sf()
```

Now create a ggplot2 object from our data:

-   `fill` is for the interior colour (e.g. polygons)

-   `color` is for borders and points

```{r}
library(ggspatial)
library(ggplot2) #loaded above but somehow got errors load again here
```

```{r}
# Create the map
ggplot() +
  annotation_map_tile(zoom = 13) +
  geom_sf(data = BluePlaquesSubPoints_sf, aes(color=dbcluster), size = 3)+
  geom_sf(data = chull_polygons, aes(fill = dbcluster), 
          alpha = 0.8, 
          # remove polygon borders
          color = NA, 
          show.legend = FALSE) +

  theme_bw()
```
